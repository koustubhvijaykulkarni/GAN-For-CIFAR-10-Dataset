{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SelfAttention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlnX04tJGtJd",
        "colab_type": "text"
      },
      "source": [
        "SAGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ordIrfDGb27",
        "colab_type": "code",
        "outputId": "cc697720-50d3-48f5-e859-c7faacb16666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras.layers import  InputSpec\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_VdHo37H3a3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttention(Layer):\n",
        "\n",
        "    def __init__(self, ch, **kwargs):\n",
        "        super(SelfAttention, self).__init__(**kwargs)\n",
        "        self.channels = ch\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        # Create a trainable weight variable for this layer:\n",
        "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True) #This is gamma parameter which is learnable parameter, initially set to 0\n",
        "        self.f = self.add_weight(shape=(1,1,self.channels,self.channels//8),initializer='glorot_uniform',name='kernel_f',trainable=True)\n",
        "        self.g = self.add_weight(shape=(1,1,self.channels,self.channels//8),initializer='glorot_uniform',name='kernel_g',trainable=True)\n",
        "        self.h = self.add_weight(shape=(1,1,self.channels,self.channels),initializer='glorot_uniform',name='kernel_h',trainable=True)\n",
        "\n",
        "        super(SelfAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        def hw_flatten(x):\n",
        "            #print(K.int_shape(x))\n",
        "            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[3]])\n",
        "\n",
        "        f = K.conv2d(x,kernel=self.f,strides=(1, 1), padding='same')  \n",
        "        g = K.conv2d(x,kernel=self.g,strides=(1, 1), padding='same')  \n",
        "        h = K.conv2d(x,kernel=self.h,strides=(1, 1), padding='same')  \n",
        "        #print(K.int_shape(f))\n",
        "        #print(K.int_shape(g))\n",
        "        #print(K.int_shape(h))\n",
        "        s = K.batch_dot(hw_flatten(g), K.permute_dimensions(hw_flatten(f), (0, 2, 1))) \n",
        "\n",
        "        beta = K.softmax(s, axis=-1)  # attention map\n",
        "        #pyplot.matshow(K.eval(beta))\n",
        "        #puplot.show()\n",
        "        o = K.batch_dot(beta, hw_flatten(h))  \n",
        "\n",
        "        o = K.reshape(o, shape=K.shape(x))  \n",
        "        x = self.gamma * o + x\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}